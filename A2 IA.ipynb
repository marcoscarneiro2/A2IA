{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d2e29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marcos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Marcos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Marcos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "nltk.download( 'punkt')\n",
    "nltk.download( 'wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53c541fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 2\n",
    "# inicializaremos nossa lista de palavras, classes, documentos e\n",
    "# definimos quais palavras serão ignoradas\n",
    "words = []\n",
    "documents = []\n",
    "# é feita a leitura do arquivo intents.json e transformado em json\n",
    "intents = json.loads(open('intents.json').read())\n",
    "# adicionamos as tags em nossa lista de classes\n",
    "classes = [i['tag'] for i in intents['intencoes']]\n",
    "ignore_words = [\"!\", \"@\", \"#\", \"$\", \"%\", \"*\", \"?\"]\n",
    "\n",
    "# percorremos nosso array de objetos\n",
    "for intent in intents['intencoes']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # com ajuda no nltk fazemos aqui a tokenizaçao dos patterns\n",
    "        # e adicionamos na lista de palavras\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)\n",
    "\n",
    "        # adiciona aos documentos para identificarmos a tag para a mesma\n",
    "        documents.append((word, intent['tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f87b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 3\n",
    "# lematizamos as palavras ignorando as palavras da lista ignore_words\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "\n",
    "# classificamos nossas listas\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# salvamos as palavras e classes nos arquivos pkl\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "def82afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 4\n",
    "# Criar base de dados para o treinamento\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for document in documents:\n",
    "    # inicializamos o saco de palavras\n",
    "    bag = []\n",
    "    # listamos as palavras do pattern\n",
    "    pattern_words = document[0]\n",
    "    # lematizamos cada palavra\n",
    "    # na tentativa de representar palavras relacionadas\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "\n",
    "    # criamos nosso conjunto de palavras com 1,\n",
    "    # se a correspondência de palavras for encontrada no padrão atual\n",
    "    for word in words:\n",
    "        bag.append(1) if word in pattern_words else bag.append(0)\n",
    "\n",
    "    # output_row atuará como uma chave para a lista,\n",
    "    # onde a saída será 0 para cada tag e 1 para a tag atual\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# embaralhamos nosso conjunto de treinamentos e transformamos em numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "# criamos lista de treino sendo x os patterns e y as intenções\n",
    "x = list(training[:, 0])\n",
    "y = list(training[:, 1])\n",
    "\n",
    "# Salvar x e y como arquivos pickle\n",
    "pickle.dump(x, open('x.pkl', 'wb'))\n",
    "pickle.dump(y, open('y.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5a0ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criamos nosso conjunto de palavras com 1,\n",
    "# se a correspondência de palavras for encontrada no padrão atual\n",
    "for word in words:\n",
    "    bag.append(1) if word in pattern_words else bag.append(0)\n",
    "\n",
    "# Create output row\n",
    "output_row = list(output_empty)\n",
    "output_row[classes.index(document[1])] = 1\n",
    "\n",
    "# Create new_element with the modified dimension\n",
    "new_element = np.array([bag, output_row], dtype=object)\n",
    "new_element = new_element[np.newaxis, :]  # Add an extra dimension\n",
    "\n",
    "# Concatenate new_element with training\n",
    "training = np.concatenate((training, new_element), axis=0)\n",
    "\n",
    "# Shuffle and convert training to numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# Create x and y lists for training\n",
    "x = list(training[:, 0])\n",
    "y = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f92c2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def generate_pdf(data_lists):\n",
    "    pdf_filename = \"data_lists.pdf\"\n",
    "\n",
    "    # Inicializar o objeto Canvas\n",
    "    c = canvas.Canvas(pdf_filename, pagesize=letter)\n",
    "\n",
    "    # Definir as posições iniciais para escrever as listas\n",
    "    x = 50\n",
    "    y = 750\n",
    "\n",
    "    # Escrever a lista de palavras (words)\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    c.drawString(x, y, \"Lista de Palavras (words):\")\n",
    "    y -= 20\n",
    "    for word in data_lists[\"words\"]:\n",
    "        c.drawString(x, y, word)\n",
    "        y -= 15\n",
    "\n",
    "    # Escrever a lista de classes\n",
    "    y -= 20\n",
    "    c.drawString(x, y, \"Lista de Classes:\")\n",
    "    y -= 20\n",
    "    for class_item in data_lists[\"classes\"]:\n",
    "        c.drawString(x, y, class_item)\n",
    "        y -= 15\n",
    "\n",
    "    # Escrever a lista de dados de treino X\n",
    "    y -= 20\n",
    "    c.drawString(x, y, \"Lista de Dados de Treino X:\")\n",
    "    y -= 20\n",
    "    for x_item in data_lists[\"x\"]:\n",
    "        c.drawString(x, y, str(x_item))\n",
    "        y -= 15\n",
    "\n",
    "    # Escrever a lista de alvos de treino Y\n",
    "    y -= 20\n",
    "    c.drawString(x, y, \"Lista de Alvos de Treino Y:\")\n",
    "    y -= 20\n",
    "    for y_item in data_lists[\"y\"]:\n",
    "        c.drawString(x, y, str(y_item))\n",
    "        y -= 15\n",
    "\n",
    "    # Salvar e fechar o arquivo PDF\n",
    "    c.save()\n",
    "\n",
    "# Obter as listas de dados\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "x = pickle.load(open('x.pkl', 'rb'))\n",
    "y = pickle.load(open('y.pkl', 'rb'))\n",
    "\n",
    "# Criar um dicionário com as listas de dados\n",
    "data_lists = {\n",
    "    \"words\": words,\n",
    "    \"classes\": classes,\n",
    "    \"x\": x,\n",
    "    \"y\": y,\n",
    "}\n",
    "\n",
    "# Gerar o PDF com as listas de dados\n",
    "generate_pdf(data_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60371542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
